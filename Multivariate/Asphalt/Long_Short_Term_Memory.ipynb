{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1b1d98-c820-41d2-b86f-68ed2a4223b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# Data Transformation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def Transform(train_slices, test_slices):\n",
    "    def process_transform_dataset(df):\n",
    "        samples = []\n",
    "        m = 298\n",
    "        k = 3\n",
    "        n = len(df.index)\n",
    "        for i in range(n):\n",
    "            x = df.iloc[i].tolist()\n",
    "            sample = []\n",
    "            for j in range(m):\n",
    "                index = j \n",
    "                for p in range(k):\n",
    "                    sample.append(x[index])\n",
    "                    index = index + m\n",
    "            sample.append(x[len(x)-1])\n",
    "            samples.append(sample)\n",
    "        return samples\n",
    "\n",
    "    train_df = pd.DataFrame(train_slices)\n",
    "    test_df = pd.DataFrame(test_slices)\n",
    "\n",
    "\n",
    "    train_transform = process_transform_dataset(train_df)\n",
    "    test_transform = process_transform_dataset(test_df)\n",
    "    return train_transform, test_transform\n",
    "    \n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd93072-7f31-4970-9a82-ee12bf73899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tenzi\\anaconda3\\envs\\aeon-env\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 288ms/step - accuracy: 0.2271 - loss: 1.3997 - val_accuracy: 0.0256 - val_loss: 1.4610\n",
      "Epoch 2/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 268ms/step - accuracy: 0.2920 - loss: 1.3583 - val_accuracy: 0.3333 - val_loss: 1.4073\n",
      "Epoch 3/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 241ms/step - accuracy: 0.3528 - loss: 1.2623 - val_accuracy: 0.5641 - val_loss: 1.2029\n",
      "Epoch 4/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 247ms/step - accuracy: 0.4126 - loss: 1.2093 - val_accuracy: 0.6667 - val_loss: 1.0445\n",
      "Epoch 5/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 241ms/step - accuracy: 0.4256 - loss: 1.1737 - val_accuracy: 0.7949 - val_loss: 0.8640\n",
      "Epoch 6/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 246ms/step - accuracy: 0.4482 - loss: 1.1382 - val_accuracy: 0.8462 - val_loss: 0.9166\n",
      "Epoch 7/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 212ms/step - accuracy: 0.4724 - loss: 1.1091 - val_accuracy: 0.8462 - val_loss: 0.8576\n",
      "Epoch 8/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 211ms/step - accuracy: 0.4761 - loss: 1.1211 - val_accuracy: 0.6923 - val_loss: 1.0770\n",
      "Epoch 9/200\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 172ms/step - accuracy: 0.5049 - loss: 1.0529 - val_accuracy: 0.8974 - val_loss: 0.7310\n",
      "Epoch 10/200\n",
      "\u001b[1m18/30\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 177ms/step - accuracy: 0.5258 - loss: 1.0118"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "import random as rn\n",
    "rn.seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(os.path.join(os.getcwd(),'Input', 'AsphaltObstacle_Train.csv'))\n",
    "test = pd.read_csv(os.path.join(os.getcwd(),'Input', 'AsphaltObstacle_Test.csv'))\n",
    "\n",
    "# Trasformation\n",
    "train_transform, test_transform = Transform(train, test)\n",
    "train = pd.DataFrame(train_transform)\n",
    "test = pd.DataFrame(test_transform)\n",
    "\n",
    "# Preprocess the data\n",
    "x_train, y_train = train.iloc[:, :-1].values, train.iloc[:, -1].values\n",
    "x_test, y_test = test.iloc[:, :-1].values, test.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Feature scaling\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train).reshape(x_train.shape[0], 298, 3)\n",
    "x_test = sc.transform(x_test).reshape(x_test.shape[0], 298, 3)\n",
    "\n",
    "# Train the model\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Compute class weights for each class\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "# Create a dictionary that maps class labels to their corresponding weight\n",
    "class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Set up directories and model parameters\n",
    "base_directory = os.path.join(os.getcwd(), 'Output', 'LSTM')\n",
    "os.makedirs(base_directory, exist_ok=True)\n",
    "\n",
    "# Define model parameters and train\n",
    "m, epochs, patience = 24,200,4  # example values\n",
    "subdirectory = os.path.join(base_directory, f\"{m}.{m}.{epochs}.{patience}\")\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    LSTM(m, return_sequences=True, input_shape=(298, 3)),\n",
    "    LSTM(m),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=patience)\n",
    "cp = ModelCheckpoint(os.path.join(subdirectory, 'bestweights.keras'), monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "\n",
    "# Train the model with class weights\n",
    "model.fit(x_train, y_train, validation_split=0.1, epochs=epochs, batch_size=12, \n",
    "          callbacks=[es, cp], class_weight=class_weight_dict)\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {scores[1] * 100:.2f}%\")\n",
    "\n",
    "# Predict and calculate testing time\n",
    "start_time = time.time()\n",
    "y_pred = model.predict(x_test)\n",
    "end_time = time.time()\n",
    "testing_time = end_time - start_time\n",
    "print(f\"Testing Time: {testing_time} seconds\")\n",
    "\n",
    "# Save testing time\n",
    "with open(os.path.join(subdirectory, 'testing_time.txt'), \"w\") as f:\n",
    "    f.write(f\"Testing Time: {testing_time} seconds\")\n",
    "\n",
    "# Save class probabilities\n",
    "np.savetxt(os.path.join(subdirectory, 'class_probabilities.csv'), y_pred, delimiter=',')\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Save classification report\n",
    "report_str = classification_report(y_test_labels, y_pred_labels)\n",
    "with open(os.path.join(subdirectory, 'classification_report.txt'), \"w\") as f:\n",
    "    f.write(report_str)\n",
    "\n",
    "# Confusion matrix plot\n",
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test_labels, y_pred_labels)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "disp.figure_.savefig(os.path.join(subdirectory, 'Confusion_Matrix.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4a0ce-dd3c-41b2-a4ba-2f1a4e47f07f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
