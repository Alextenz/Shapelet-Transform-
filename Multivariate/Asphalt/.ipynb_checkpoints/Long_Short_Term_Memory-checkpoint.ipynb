{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f1b1d98-c820-41d2-b86f-68ed2a4223b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "# Data Transformation \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def Transform(train_slices, test_slices):\n",
    "    def process_transform_dataset(df):\n",
    "        samples = []\n",
    "        m = 298\n",
    "        k = 3\n",
    "        n = len(df.index)\n",
    "        for i in range(n):\n",
    "            x = df.iloc[i].tolist()\n",
    "            sample = []\n",
    "            for j in range(m):\n",
    "                index = j \n",
    "                for p in range(k):\n",
    "                    sample.append(x[index])\n",
    "                    index = index + m\n",
    "            sample.append(x[len(x)-1])\n",
    "            samples.append(sample)\n",
    "        return samples\n",
    "\n",
    "    train_df = pd.DataFrame(train_slices)\n",
    "    test_df = pd.DataFrame(test_slices)\n",
    "\n",
    "\n",
    "    train_transform = process_transform_dataset(train_df)\n",
    "    test_transform = process_transform_dataset(test_df)\n",
    "    return train_transform, test_transform\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd93072-7f31-4970-9a82-ee12bf73899d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tenzi\\anaconda3\\envs\\aeon-env\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/26\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 115ms/step - accuracy: 0.2643 - loss: 1.3870"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(1)\n",
    "import random as rn\n",
    "rn.seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv(os.path.join(os.getcwd(),'Input', 'AsphaltObstacle_Train.csv'))\n",
    "test = pd.read_csv(os.path.join(os.getcwd(),'Input', 'AsphaltObstacle_Test.csv'))\n",
    "\n",
    "# Trasformation\n",
    "train_transform, test_transform = Transform(train, test)\n",
    "train = pd.DataFrame(train_transform)\n",
    "test = pd.DataFrame(test_transform)\n",
    "\n",
    "# Preprocess the data\n",
    "x_train, y_train = train.iloc[:, :-1].values, train.iloc[:, -1].values\n",
    "x_test, y_test = test.iloc[:, :-1].values, test.iloc[:, -1].values\n",
    "\n",
    "\n",
    "# Feature scaling\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train).reshape(x_train.shape[0], 298, 3)\n",
    "x_test = sc.transform(x_test).reshape(x_test.shape[0], 298, 3)\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train = encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_test = encoder.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Set up directories and model parameters\n",
    "base_directory = os.path.join(os.getcwd(), 'Output', 'LSTM')\n",
    "os.makedirs(base_directory, exist_ok=True)\n",
    "\n",
    "# Define model parameters and train\n",
    "m, n, epochs, patience = 8,5,200,4  # example values\n",
    "subdirectory = os.path.join(base_directory, f\"{m}.{n}.{epochs}.{patience}\")\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    LSTM(m, return_sequences=True, input_shape=(298, 3)),\n",
    "    LSTM(n),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', patience=patience)\n",
    "cp = ModelCheckpoint(os.path.join(subdirectory, 'bestweights.keras'), monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, validation_split=0.2, epochs=epochs, batch_size=12, callbacks=[es, cp])\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {scores[1] * 100:.2f}%\")\n",
    "\n",
    "# Predict and calculate testing time\n",
    "start_time = time.time()\n",
    "y_pred = model.predict(x_test)\n",
    "end_time = time.time()\n",
    "testing_time = end_time - start_time\n",
    "print(f\"Testing Time: {testing_time} seconds\")\n",
    "\n",
    "# Save testing time\n",
    "with open(os.path.join(subdirectory, 'testing_time.txt'), \"w\") as f:\n",
    "    f.write(f\"Testing Time: {testing_time} seconds\")\n",
    "\n",
    "# Save class probabilities\n",
    "np.savetxt(os.path.join(subdirectory, 'class_probabilities.csv'), y_pred, delimiter=',')\n",
    "\n",
    "# Classification report and confusion matrix\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Save classification report\n",
    "report_str = classification_report(y_test_labels, y_pred_labels)\n",
    "with open(os.path.join(subdirectory, 'classification_report.txt'), \"w\") as f:\n",
    "    f.write(report_str)\n",
    "\n",
    "# Confusion matrix plot\n",
    "disp = metrics.ConfusionMatrixDisplay.from_predictions(y_test_labels, y_pred_labels)\n",
    "disp.figure_.suptitle(\"Confusion Matrix\")\n",
    "disp.figure_.savefig(os.path.join(subdirectory, 'Confusion_Matrix.png'))\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdd259-ba77-4b99-866a-b56c13be2001",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
